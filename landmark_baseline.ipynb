{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "landmark_sample_resize.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1anw1DI1t4X9_KApQlTLf_lIcS6UapAnz",
      "authorship_tag": "ABX9TyNwBQVU3jzp47jeD8FbdrT6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zbooster/Landmark-Image-Classification/blob/main/landmark_baseline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 데이터 준비\n"
      ],
      "metadata": {
        "id": "JXvxjEDBaosT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Google 드라이브에서 Resize된 데이터 가져오기\n",
        "gdown과 unzip을 이용하여 압축을 풀어서 나의 colab 폴더에 둔다"
      ],
      "metadata": {
        "id": "-Cno43UXehHN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1FSb1ahdmNENxvZvg921R47_ZfNJ0O8va"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3g9BQaWaeP9m",
        "outputId": "43982c0b-3070-4f8c-b6e6-d7c5e331ee5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1FSb1ahdmNENxvZvg921R47_ZfNJ0O8va\n",
            "To: /content/resizeds.zip\n",
            " 51% 1.58G/3.09G [00:06<00:06, 226MB/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "original_dataset_dir = '/content/datasets/resizeds/Training'\n",
        "classes_list = os.listdir(original_dataset_dir)\n",
        "\n",
        "classes_list[:2], len(classes_list)"
      ],
      "metadata": {
        "id": "tjZlUZn3amJP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "382cd2cd-40cc-4f6f-c45d-87a0baadadfe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['가재마을세종호반베르디움2단지아파트', '새뜸마을10단지더샵힐스테이트아파트'], 84)"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 데이터 정리"
      ],
      "metadata": {
        "id": "ql_vB7iwe4aB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 폴더 생성\n"
      ],
      "metadata": {
        "id": "R9js69d6baqj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_dir = './splitted'\n",
        "os.mkdir(base_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "BzDFoDOv7QBG",
        "outputId": "99157960-e4c7-4c8e-ac0a-320a2a0f23df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileExistsError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-62-7902cff60860>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mbase_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./splitted'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mFileExistsError\u001b[0m: [Errno 17] File exists: './splitted'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "os.mkdir(train_dir)\n",
        "validation_dir = os.path.join(base_dir, 'val')\n",
        "os.mkdir(validation_dir)\n",
        "\n",
        "for cls in classes_list:\n",
        "  os.mkdir(os.path.join(train_dir, cls))\n",
        "  os.mkdir(os.path.join(validation_dir, cls))"
      ],
      "metadata": {
        "id": "CkDVhioea3_2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 데이터 복사"
      ],
      "metadata": {
        "id": "xo8HUOJRfEPK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "original_dataset_dir = '/content/datasets/resizeds'\n",
        "\n",
        "for cls in classes_list:\n",
        "  train_path = os.path.join(original_dataset_dir, 'Training', cls)\n",
        "  train_fnames = os.listdir(train_path)\n",
        "\n",
        "  print(\"Train size(\", cls, \"): \", len(train_fnames))\n",
        "  for fname in train_fnames:\n",
        "    src = os.path.join(train_path, fname)\n",
        "    dst = os.path.join(os.path.join(train_dir, cls), fname)\n",
        "    shutil.copyfile(src, dst)\n",
        "\n",
        "  val_path = os.path.join(original_dataset_dir, 'Validation', cls)\n",
        "  val_fnames = os.listdir(val_path)\n",
        "  print(\"Validation size(\", cls, \"): \", len(val_fnames))\n",
        "  for fname in val_fnames:\n",
        "    src = os.path.join(val_path, fname)\n",
        "    dst = os.path.join(os.path.join(validation_dir, cls), fname)\n",
        "    shutil.copyfile(src, dst)"
      ],
      "metadata": {
        "id": "ruD--Ly36P8G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 학습준비"
      ],
      "metadata": {
        "id": "7HWPjTXTdA5u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "\n",
        "USE_CUDA = torch.cuda.is_available()\n",
        "DEVICE = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n",
        "BATCH_SIZE = 256\n",
        "EPOCH = 30"
      ],
      "metadata": {
        "id": "0kOee1DSc5Pf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "\n",
        "transform_base = transforms.Compose([transforms.Resize((64, 64)), transforms.ToTensor()])\n",
        "train_dataset = ImageFolder(root='./splitted/train/', transform=transform_base)\n",
        "val_dataset = ImageFolder(root='./splitted/val/', transform=transform_base)"
      ],
      "metadata": {
        "id": "CSlqkaXRdIE8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
        "                                           batch_size=BATCH_SIZE,\n",
        "                                           shuffle=True,\n",
        "                                           num_workers=2)\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset,\n",
        "                                         batch_size=BATCH_SIZE,\n",
        "                                         shuffle=True,\n",
        "                                         num_workers=2)"
      ],
      "metadata": {
        "id": "NANTZBs3dX0d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "class Net(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "\n",
        "    super(Net, self).__init__()\n",
        "\n",
        "    self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n",
        "    self.pool = nn.MaxPool2d(2, 2)\n",
        "    self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
        "    self.conv3 = nn.Conv2d(64, 64, 3, padding=1)\n",
        "\n",
        "    self.fc1 = nn.Linear(4096, 512)\n",
        "    self.fc2 = nn.Linear(512, 84)\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    x = self.conv1(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.pool(x)\n",
        "    x = F.dropout(x, p=0.25, training=self.training)\n",
        "\n",
        "    x = self.conv2(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.pool(x)\n",
        "    x = F.dropout(x, p=0.25, training=self.training)\n",
        "\n",
        "    x = self.conv3(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.pool(x)\n",
        "    x = F.dropout(x, p=0.25, training=self.training)\n",
        "\n",
        "    x = x.view(-1, 4096)\n",
        "    x = self.fc1(x)\n",
        "    x = F.relu(x)\n",
        "    x = F.dropout(x, p=0.5, training=self.training)\n",
        "    x = self.fc2(x)\n",
        "\n",
        "    return F.log_softmax(x, dim=1)"
      ],
      "metadata": {
        "id": "fCklg83QeK_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_base = Net().to(DEVICE)\n",
        "optimizer = optim.Adam(model_base.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "vFk2g-Dkf-9O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, train_loader, optimizer):\n",
        "  model.train()\n",
        "  for batch_idx, (data, target) in enumerate(train_loader):\n",
        "    data, target = data.to(DEVICE), target.to(DEVICE)\n",
        "    optimizer.zero_grad()\n",
        "    output = model(data)\n",
        "    loss = F.cross_entropy(output, target)\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "id": "xj3Ac3BBgIF1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, test_loader):\n",
        "  model.eval()\n",
        "  test_loss = 0\n",
        "  correct = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for data, target in test_loader:\n",
        "      data, target =  data.to(DEVICE), target.to(DEVICE)\n",
        "      output = model(data)\n",
        "\n",
        "      test_loss += F.cross_entropy(output, target, reduction='sum').item()\n",
        "\n",
        "      pred = output.max(1, keepdim=True)[1]\n",
        "      correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "  test_loss /= len(test_loader.dataset)\n",
        "  test_accuracy = 100. * correct / len(test_loader.dataset)\n",
        "  return test_loss, test_accuracy"
      ],
      "metadata": {
        "id": "GhXjI5ngg2Ru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import copy\n",
        "\n",
        "def train_baseline(model, train_loader, val_loader, optimizer, num_epochs=30):\n",
        "  best_acc = 0.0\n",
        "  best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "  for epoch in range(1, num_epochs + 1):\n",
        "    since = time.time()\n",
        "    train(model, train_loader, optimizer)\n",
        "    train_loss, train_acc = evaluate(model, train_loader)\n",
        "    val_loss, val_acc = evaluate(model, val_loader)\n",
        "\n",
        "    if val_acc > best_acc:\n",
        "      best_acc = val_acc\n",
        "      best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('----------------- epoch {} ------------------'.format(epoch))\n",
        "    print('train Loss: {:.4f}, Accuracy: {:.2f}%'.format(train_loss, train_acc))\n",
        "    print('val Loss: {:.4f}, Accuracy: {:.2f}%'.format(val_loss, val_acc))\n",
        "    print('Completed in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "\n",
        "  model.load_state_dict(best_model_wts)\n",
        "  return model"
      ],
      "metadata": {
        "id": "AwsUXDQShs6O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 학습 및 저장"
      ],
      "metadata": {
        "id": "-NXXOT5EfOJW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base = train_baseline(model_base, train_loader, val_loader, optimizer, EPOCH)\n",
        "torch.save(base, 'baseline.pt')"
      ],
      "metadata": {
        "id": "J52OivOTjVOG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}