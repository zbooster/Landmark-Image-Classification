{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "landmark_sample_resize.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1anw1DI1t4X9_KApQlTLf_lIcS6UapAnz",
      "authorship_tag": "ABX9TyOtPfGxZfT0c+ysrm2PyMHz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zbooster/Landmark-Image-Classification/blob/main/landmark_baseline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 데이터 준비\n"
      ],
      "metadata": {
        "id": "JXvxjEDBaosT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Google 드라이브에서 Resize된 데이터 가져오기\n",
        "gdown과 unzip을 이용하여 압축을 풀어서 나의 colab 폴더에 둔다"
      ],
      "metadata": {
        "id": "-Cno43UXehHN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1FSb1ahdmNENxvZvg921R47_ZfNJ0O8va"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3g9BQaWaeP9m",
        "outputId": "43982c0b-3070-4f8c-b6e6-d7c5e331ee5b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1FSb1ahdmNENxvZvg921R47_ZfNJ0O8va\n",
            "To: /content/resizeds.zip\n",
            "100% 3.09G/3.09G [00:12<00:00, 238MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -qq '/content/resizeds.zip' -d './datasets'"
      ],
      "metadata": {
        "id": "xnkamQmFiJG9"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 학습준비"
      ],
      "metadata": {
        "id": "7HWPjTXTdA5u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "\n",
        "USE_CUDA = torch.cuda.is_available()\n",
        "DEVICE = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n",
        "BATCH_SIZE = 256\n",
        "EPOCH = 30"
      ],
      "metadata": {
        "id": "0kOee1DSc5Pf"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "\n",
        "transform_base = transforms.Compose([transforms.Resize((64, 64)), transforms.ToTensor()])\n",
        "train_dataset = ImageFolder(root='/content/datasets/resizeds/Training', transform=transform_base)\n",
        "val_dataset = ImageFolder(root='/content/datasets/resizeds/Validation', transform=transform_base)"
      ],
      "metadata": {
        "id": "CSlqkaXRdIE8"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
        "                                           batch_size=BATCH_SIZE,\n",
        "                                           shuffle=True,\n",
        "                                           num_workers=2)\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset,\n",
        "                                         batch_size=BATCH_SIZE,\n",
        "                                         shuffle=True,\n",
        "                                         num_workers=2)"
      ],
      "metadata": {
        "id": "NANTZBs3dX0d"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "class Net(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "\n",
        "    super(Net, self).__init__()\n",
        "\n",
        "    self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n",
        "    self.pool = nn.MaxPool2d(2, 2)\n",
        "    self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
        "    self.conv3 = nn.Conv2d(64, 64, 3, padding=1)\n",
        "\n",
        "    self.fc1 = nn.Linear(4096, 512)\n",
        "    self.fc2 = nn.Linear(512, 84)\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    x = self.conv1(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.pool(x)\n",
        "    x = F.dropout(x, p=0.25, training=self.training)\n",
        "\n",
        "    x = self.conv2(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.pool(x)\n",
        "    x = F.dropout(x, p=0.25, training=self.training)\n",
        "\n",
        "    x = self.conv3(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.pool(x)\n",
        "    x = F.dropout(x, p=0.25, training=self.training)\n",
        "\n",
        "    x = x.view(-1, 4096)\n",
        "    x = self.fc1(x)\n",
        "    x = F.relu(x)\n",
        "    x = F.dropout(x, p=0.5, training=self.training)\n",
        "    x = self.fc2(x)\n",
        "\n",
        "    return F.log_softmax(x, dim=1)"
      ],
      "metadata": {
        "id": "fCklg83QeK_H"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_base = Net().to(DEVICE)\n",
        "optimizer = optim.Adam(model_base.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "vFk2g-Dkf-9O"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, train_loader, optimizer):\n",
        "  model.train()\n",
        "  for batch_idx, (data, target) in enumerate(train_loader):\n",
        "    data, target = data.to(DEVICE), target.to(DEVICE)\n",
        "    optimizer.zero_grad()\n",
        "    output = model(data)\n",
        "    loss = F.cross_entropy(output, target)\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "id": "xj3Ac3BBgIF1"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, test_loader):\n",
        "  model.eval()\n",
        "  test_loss = 0\n",
        "  correct = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for data, target in test_loader:\n",
        "      data, target =  data.to(DEVICE), target.to(DEVICE)\n",
        "      output = model(data)\n",
        "\n",
        "      test_loss += F.cross_entropy(output, target, reduction='sum').item()\n",
        "\n",
        "      pred = output.max(1, keepdim=True)[1]\n",
        "      correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "  test_loss /= len(test_loader.dataset)\n",
        "  test_accuracy = 100. * correct / len(test_loader.dataset)\n",
        "  return test_loss, test_accuracy"
      ],
      "metadata": {
        "id": "GhXjI5ngg2Ru"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import copy\n",
        "\n",
        "def train_baseline(model, train_loader, val_loader, optimizer, num_epochs=30):\n",
        "  best_acc = 0.0\n",
        "  best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "  for epoch in range(1, num_epochs + 1):\n",
        "    since = time.time()\n",
        "    train(model, train_loader, optimizer)\n",
        "    train_loss, train_acc = evaluate(model, train_loader)\n",
        "    val_loss, val_acc = evaluate(model, val_loader)\n",
        "\n",
        "    if val_acc > best_acc:\n",
        "      best_acc = val_acc\n",
        "      best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('----------------- epoch {} ------------------'.format(epoch))\n",
        "    print('train Loss: {:.4f}, Accuracy: {:.2f}%'.format(train_loss, train_acc))\n",
        "    print('val Loss: {:.4f}, Accuracy: {:.2f}%'.format(val_loss, val_acc))\n",
        "    print('Completed in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "\n",
        "  model.load_state_dict(best_model_wts)\n",
        "  return model"
      ],
      "metadata": {
        "id": "AwsUXDQShs6O"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 학습 및 저장"
      ],
      "metadata": {
        "id": "-NXXOT5EfOJW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base = train_baseline(model_base, train_loader, val_loader, optimizer, EPOCH)\n",
        "torch.save(base, 'baseline.pt')"
      ],
      "metadata": {
        "id": "J52OivOTjVOG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63c84861-3ab1-44bd-eee0-045e97905f18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------- epoch 1 ------------------\n",
            "train Loss: 2.7647, Accuracy: 34.78%\n",
            "val Loss: 2.8902, Accuracy: 28.12%\n",
            "Completed in 3m 54s\n",
            "----------------- epoch 2 ------------------\n",
            "train Loss: 1.2971, Accuracy: 68.16%\n",
            "val Loss: 1.6799, Accuracy: 54.12%\n",
            "Completed in 4m 0s\n",
            "----------------- epoch 3 ------------------\n",
            "train Loss: 0.8153, Accuracy: 80.36%\n",
            "val Loss: 1.3218, Accuracy: 64.96%\n",
            "Completed in 3m 46s\n",
            "----------------- epoch 4 ------------------\n",
            "train Loss: 0.4756, Accuracy: 88.54%\n",
            "val Loss: 1.0017, Accuracy: 75.27%\n",
            "Completed in 3m 45s\n",
            "----------------- epoch 5 ------------------\n",
            "train Loss: 0.3352, Accuracy: 92.43%\n",
            "val Loss: 0.8064, Accuracy: 79.65%\n",
            "Completed in 3m 43s\n",
            "----------------- epoch 6 ------------------\n",
            "train Loss: 0.2535, Accuracy: 94.42%\n",
            "val Loss: 0.8334, Accuracy: 80.92%\n",
            "Completed in 3m 42s\n",
            "----------------- epoch 7 ------------------\n",
            "train Loss: 0.1966, Accuracy: 96.13%\n",
            "val Loss: 0.7034, Accuracy: 82.38%\n",
            "Completed in 3m 41s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "y_pred = []\n",
        "y_true = []\n",
        "\n",
        "# iterate over test data\n",
        "for inputs, labels in val_loader:\n",
        "        output = model_base(inputs) # Feed Network\n",
        "\n",
        "        output = (torch.max(torch.exp(output), 1)[1]).data.cpu().numpy()\n",
        "        y_pred.extend(output) # Save Prediction\n",
        "        \n",
        "        labels = labels.data.cpu().numpy()\n",
        "        y_true.extend(labels) # Save Truth\n",
        "\n",
        "# constant for classes\n",
        "classes = val_dataset.classes\n",
        "\n",
        "# Build confusion matrix\n",
        "cf_matrix = confusion_matrix(y_true, y_pred)\n",
        "df_cm = pd.DataFrame(cf_matrix/np.sum(cf_matrix) *10, index = [i for i in classes],\n",
        "                     columns = [i for i in classes])\n",
        "plt.figure(figsize = (12,7))\n",
        "sn.heatmap(df_cm, annot=True)\n",
        "plt.savefig('output.png')"
      ],
      "metadata": {
        "id": "eyN--GRXivg3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}